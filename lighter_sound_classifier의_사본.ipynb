{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PUTG/AntiSmokingAssistant/blob/main/lighter_sound_classifier%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XX46cTrh6iD"
      },
      "source": [
        "Copyright 2021 The TensorFlow Hub Authors.\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Google Drive 마운트\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3Gj1t4tdR_Ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/requirements.txt'\n",
        "\n",
        "# 파일 읽어오기\n",
        "with open(file_path, 'r') as file:\n",
        "    requirements = file.read()\n",
        "\n",
        "print(requirements)\n"
      ],
      "metadata": {
        "id": "iuW27GZxjFDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r '/content/drive/MyDrive/requirements.txt'\n"
      ],
      "metadata": {
        "id": "CsrZDNsdkLmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tflite-model-maker==0.4.2 --no-dependencies"
      ],
      "metadata": {
        "id": "0YoI-t-_jOLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflowjs"
      ],
      "metadata": {
        "id": "rgorlLXpjeXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwUA9u4oWoCR"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tflite_model_maker as mm\n",
        "from tflite_model_maker import audio_classifier\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import itertools\n",
        "import glob\n",
        "import random\n",
        "\n",
        "from IPython.display import Audio, Image\n",
        "from scipy.io import wavfile\n",
        "\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "print(f\"Model Maker Version: {mm.__version__}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upNRfilkNSmr"
      },
      "source": [
        "import pathlib\n",
        "\n",
        "DATASET_PATH =  '/content/drive/MyDrive/lighter'  # 라이터 데이터셋 폴더 경로\n",
        "data_dir = pathlib.Path(DATASET_PATH)\n",
        "\n",
        "if not data_dir.exists():\n",
        "  lighter_dataset_folder = tf.keras.utils.get_file('lighter.zip',\n",
        "                                                'https://drive.google.com/drive/folders/1debtw0CY8HdnT1HZsMDa7OhWJDQVIHNr?usp=sharing',\n",
        "                                                cache_dir='./',\n",
        "                                                cache_subdir='dataset',\n",
        "                                                extract=True)\n",
        "\n",
        "train_ds, val_ds = tf.keras.utils.audio_dataset_from_directory(\n",
        "    directory=data_dir,\n",
        "    batch_size=1,\n",
        "    validation_split=0.3,\n",
        "    seed=0,\n",
        "    output_sequence_length=16000,\n",
        "    subset='both')\n",
        "\n",
        "class_names = os.listdir(DATASET_PATH)\n",
        "print(\"클래스 목록:\", class_names)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "# 데이터셋 폴더 경로\n",
        "DATASET_PATH = '/content/drive/MyDrive/lighter'\n",
        "\n",
        "# 훈련 데이터셋 경로\n",
        "train_lighter_path = os.path.join(DATASET_PATH, 'train/turbo_train/*.wav')\n",
        "train_background_path = os.path.join(DATASET_PATH, 'train/background_train/*.wav')\n",
        "\n",
        "# 테스트 데이터셋 경로\n",
        "test_lighter_path = os.path.join(DATASET_PATH, 'test/turbo_test/*.wav')\n",
        "test_background_path = os.path.join(DATASET_PATH, 'test/background_test/*.wav')\n",
        "\n",
        "# 훈련 데이터셋 파일 목록 출력\n",
        "print(\"훈련 데이터셋 디렉토리의 파일 목록:\")\n",
        "train_lighter_files = glob.glob(train_lighter_path)\n",
        "for file_path in train_lighter_files:\n",
        "    print(file_path)\n",
        "\n",
        "# 훈련 배경음 파일 목록 출력\n",
        "print(\"\\n훈련 배경음 디렉토리의 파일 목록:\")\n",
        "train_background_files = glob.glob(train_background_path)\n",
        "for file_path in train_background_files:\n",
        "    print(file_path)\n",
        "\n",
        "# 테스트 데이터셋 파일 목록 출력\n",
        "print(\"\\n테스트 데이터셋 디렉토리의 파일 목록:\")\n",
        "test_lighter_files = glob.glob(test_lighter_path)\n",
        "for file_path in test_lighter_files:\n",
        "    print(file_path)\n",
        "\n",
        "# 테스트 배경음 파일 목록 출력\n",
        "print(\"\\n테스트 배경음 디렉토리의 파일 목록:\")\n",
        "test_background_files = glob.glob(test_background_path)\n",
        "for file_path in test_background_files:\n",
        "    print(file_path)\n"
      ],
      "metadata": {
        "id": "MaizSMZVZajB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayd7UqCfQQFU"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "from scipy.io import wavfile\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "lighter_code_to_name = {\n",
        "    'turbo_lighter': 'lighter',\n",
        "    'background': 'BackgroundSound'\n",
        "}\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/lighter'\n",
        "\n",
        "test_files = os.path.join(data_dir, 'turbo_lighter/*.wav')\n",
        "\n",
        "def get_random_audio_file():\n",
        "    test_list = glob.glob(test_files)\n",
        "    if not test_list:\n",
        "        print(\"No audio files found.\")\n",
        "        return None\n",
        "    random_audio_path = random.choice(test_list)\n",
        "    return random_audio_path\n",
        "\n",
        "def get_random_audio_by_class(data_dir, class_name, split='train'):\n",
        "    class_path = os.path.join(data_dir, f'{split}/{class_name.lower()}_{split}/*.wav')\n",
        "    class_files = glob.glob(class_path)\n",
        "\n",
        "    if not class_files:\n",
        "        print(f'No audio files found for class: {class_name}, split: {split}')\n",
        "        return None\n",
        "\n",
        "    random_audio_path = random.choice(class_files)\n",
        "    return random_audio_path\n",
        "\n",
        "def show_lighter_data(audio_path):\n",
        "    if audio_path is None:\n",
        "        return\n",
        "\n",
        "    sample_rate, audio_data = wavfile.read(audio_path)\n",
        "\n",
        "    lighter_code = audio_path.split('/')[-2]\n",
        "    print(f'lighter name: {lighter_code_to_name.get(lighter_code, \"Unknown\")}')\n",
        "    print(f'lighter code: {lighter_code}')\n",
        "\n",
        "    plttitle = f'{lighter_code_to_name.get(lighter_code, \"Unknown\")} ({lighter_code})'\n",
        "    plt.title(plttitle)\n",
        "    plt.plot(audio_data)\n",
        "    display(Audio(audio_data, rate=sample_rate))\n",
        "\n",
        "print('Functions and data structures created')\n",
        "\n",
        "# 트레인 데이터셋에서 turbo_lighter 클래스의 랜덤한 오디오 선택 및 표시\n",
        "random_audio_turbo_lighter_train = get_random_audio_by_class(data_dir, 'turbo_lighter', split='train')\n",
        "show_lighter_data(random_audio_turbo_lighter_train)\n",
        "\n",
        "# 테스트 데이터셋에서 background 클래스의 랜덤한 오디오 선택 및 표시\n",
        "random_audio_background_test = get_random_audio_by_class(data_dir, 'background', split='test')\n",
        "show_lighter_data(random_audio_background_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Before function call')\n",
        "random_audio = get_random_audio_file()\n",
        "show_lighter_data(random_audio)\n",
        "print('After function call')\n",
        "\n"
      ],
      "metadata": {
        "id": "0-iWx4LfaY5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yrv0uD7aXYl4"
      },
      "source": [
        "### Playing some audio\n",
        "\n",
        "To have a better understanding about the data, lets listen to a random audio files from the test split.\n",
        "\n",
        "Note: later in this notebook you'll run inference on this audio for testing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_files = os.path.join(DATASET_PATH, 'turbo_lighter/*.wav')\n"
      ],
      "metadata": {
        "id": "UB_IDNGra33o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEeMZh-VQy97"
      },
      "source": [
        "random_audio = get_random_audio_file()\n",
        "show_lighter_data(random_audio)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUcxtfHXY7XS"
      },
      "source": [
        "spec = audio_classifier.YamNetSpec(\n",
        "    keep_yamnet_and_custom_heads=True,\n",
        "    frame_step=3 * audio_classifier.YamNetSpec.EXPECTED_WAVEFORM_LENGTH,\n",
        "    frame_length=6 * audio_classifier.YamNetSpec.EXPECTED_WAVEFORM_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF185yZ_M7zu"
      },
      "source": [
        "## Loading the data\n",
        "\n",
        "Model Maker has the API to load the data from a folder and have it in the expected format for the model spec.\n",
        "\n",
        "The train and test split are based on the folders. The validation dataset will be created as 20% of the train split.\n",
        "\n",
        "Note: The `cache=True` is important to make training later faster but it will also require more RAM to hold the data. For the birds dataset that is not a problem since it's only 300MB, but if you use your own data you have to pay attention to it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cX0RqETqZgzo"
      },
      "source": [
        "# 데이터셋 경로 설정\n",
        "train_data = audio_classifier.DataLoader.from_folder(\n",
        "    spec, '/content/drive/MyDrive/lighter/train', cache=True)\n",
        "train_data, validation_data = train_data.split(0.8)\n",
        "test_data = audio_classifier.DataLoader.from_folder(\n",
        "    spec, '/content/drive/MyDrive/lighter/test', cache=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziMghju-Rts2"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "the audio_classifier has the [`create`](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/audio_classifier/create) method that creates a model and already start training it.\n",
        "\n",
        "You can customize many parameterss, for more information you can read more details in the documentation.\n",
        "\n",
        "On this first try you'll use all the default configurations and train for 100 epochs.\n",
        "\n",
        "Note: The first epoch takes longer than all the other ones because it's when the cache is created. After that each epoch takes close to 1 second."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r6Awvl4ZkIv"
      },
      "source": [
        "batch_size = 1\n",
        "epochs = 100\n",
        "\n",
        "print('Training the model')\n",
        "model = audio_classifier.create(\n",
        "    train_data,\n",
        "    spec,\n",
        "    validation_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXMEHZkAxJTl"
      },
      "source": [
        "The accuracy looks good but it's important to run the evaluation step on the test data and vefify your model achieved good results on unseed data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEM1aRNKtvHS"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDoQACMrZnOx"
      },
      "source": [
        "print('Evaluating the model')\n",
        "model.evaluate(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqB3c0368iH3"
      },
      "source": [
        "def show_confusion_matrix(confusion, test_labels):\n",
        "  \"\"\"Compute confusion matrix and normalize.\"\"\"\n",
        "  confusion_normalized = confusion.astype(\"float\") / confusion.sum(axis=1)\n",
        "  axis_labels = test_labels\n",
        "  ax = sns.heatmap(\n",
        "      confusion_normalized, xticklabels=axis_labels, yticklabels=axis_labels,\n",
        "      cmap='Blues', annot=True, fmt='.2f', square=True)\n",
        "  plt.title(\"Confusion matrix\")\n",
        "  plt.ylabel(\"True label\")\n",
        "  plt.xlabel(\"Predicted label\")\n",
        "\n",
        "confusion_matrix = model.confusion_matrix(test_data)\n",
        "show_confusion_matrix(confusion_matrix.numpy(), test_data.index_to_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmlmTl42Bq_u"
      },
      "source": [
        "serving_model = model.create_serving_model()\n",
        "\n",
        "print(f'Model\\'s input shape and type: {serving_model.inputs}')\n",
        "print(f'Model\\'s output shape and type: {serving_model.outputs}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uixOfKSUj_9m"
      },
      "source": [
        "The model created has a fixed input window.\n",
        "\n",
        "For a given audio file, you'll have to split it in windows of data of the expected size. The last window might need to be filled with zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAvGKQL0lNty"
      },
      "source": [
        "sample_rate, audio_data = wavfile.read(random_audio, 'rb')\n",
        "\n",
        "audio_data = np.array(audio_data) / tf.int16.max\n",
        "input_size = serving_model.input_shape[1]\n",
        "\n",
        "splitted_audio_data = tf.signal.frame(audio_data, input_size, input_size, pad_end=True, pad_value=0)\n",
        "\n",
        "print(f'Test audio path: {random_audio}')\n",
        "print(f'Original size of the audio data: {len(audio_data)}')\n",
        "print(f'Number of windows for inference: {len(splitted_audio_data)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLxKd0eFkMcR"
      },
      "source": [
        "You'll loop over all the splitted audio and apply the model for each one of them.\n",
        "\n",
        "The model you've just trained has 2 outputs: The original YAMNet's output and the one you've just trained. This is important because the real world environment is more complicated than just bird sounds. You can use the YAMNet's output to filter out non relevant audio, for example, on the birds use case, if YAMNet is not classifying Birds or Animals, this might show that the output from your model might have an irrelevant classification.\n",
        "\n",
        "\n",
        "Below both outpus are printed to make it easier to understand their relation. Most of the mistakes that your model make are when YAMNet's prediction is not related to your domain (eg: birds)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-8fJLrxGwYT"
      },
      "source": [
        "print(random_audio)\n",
        "\n",
        "results = []\n",
        "print('Result of the window ith:  your model class -> score,  (spec class -> score)')\n",
        "for i, data in enumerate(splitted_audio_data):\n",
        "  yamnet_output, inference = serving_model(data)\n",
        "  results.append(inference[0].numpy())\n",
        "  result_index = tf.argmax(inference[0])\n",
        "  spec_result_index = tf.argmax(yamnet_output[0])\n",
        "  t = spec._yamnet_labels()[spec_result_index]\n",
        "  result_str = f'Result of the window {i}: ' \\\n",
        "  f'\\t{test_data.index_to_label[result_index]} -> {inference[0][result_index].numpy():.3f}, ' \\\n",
        "  f'\\t({spec._yamnet_labels()[spec_result_index]} -> {yamnet_output[0][spec_result_index]:.3f})'\n",
        "  print(result_str)\n",
        "\n",
        "\n",
        "results_np = np.array(results)\n",
        "mean_results = results_np.mean(axis=0)\n",
        "result_index = mean_results.argmax()\n",
        "print(f'Mean result: {test_data.index_to_label[result_index]} -> {mean_results[result_index]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yASrikBgZ9ZO"
      },
      "source": [
        "## Exporting the model\n",
        "\n",
        "The last step is exporting your model to be used on embedded devices or on the browser.\n",
        "\n",
        "The `export` method export both formats for you."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 내보낼 디렉토리 경로 설정 (예시: Google 드라이브의 \"lighters_models\" 폴더)\n",
        "models_path = '/content/drive/MyDrive/lighters_models'\n"
      ],
      "metadata": {
        "id": "uHzO_6reR8cZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw_ehPxAdQlz"
      },
      "source": [
        "\n",
        "model.export(models_path, tflite_filename='lighter_model.tflite')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjZRKmurA3y_"
      },
      "source": [
        "You can also export the SavedModel version for serving or using on a Python environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xr0idac6xfi"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "You did it.\n",
        "\n",
        "Now your new model can be deployed on  mobile devices using [TFLite Audio Tasks API](https://www.tensorflow.org/lite/inference_with_metadata/task_library/overview).\n",
        "\n",
        "You can also try the same process with your own data with different classes and here is the documentation for [Model Maker for Audio Classification](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/audio_classifier).\n",
        "\n",
        "This notebook is part of the [ODML Learning path: Customizing an Audio Model](https://codelabs.developers.google.com/codelabs/tflite-audio-classification-custom-model-android). If you need more information follow the rest of the codelabs to have a clear understanding on how to use Machine Learning for Audio Classification.\n",
        "\n",
        "For more information about Model Maker for Audio Classification"
      ]
    }
  ]
}